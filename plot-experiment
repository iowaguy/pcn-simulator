#!/usr/bin/env python3

import experiments
import simulation_common
import sys

bucket_fields = ['data_set_name', 'concurrent_transactions_count', \
                 'routing_algorithms', 'arrival_delay_ms']
attack_property_fields = ['attack_type', 'attackers', 'receiver_delay_ms']

all_fields = {'general': bucket_fields, 'attack_properties': attack_property_fields}

def field_assertion(config_a, config_b):
    '''There should never be a case where a field is undefined in only one of the
    configs'''
    for field in bucket_fields:
        assert ((field in config_a) and (field in config_b)) or \
        ((field not in config_a) and (field not in config_b))

    for field in attack_property_fields:
        assert ((field in config_a) and (field in config_b)) or \
        ((field not in config_a) and (field not in config_b))

def get_config(config, second_level=None):
    if second_level and second_level in config:
        return config[second_level]
    else:
        return config
    
def compare_two_configs_single_level(config_a, config_b, second_level=None):
    field_assertion(get_config(config_a, second_level), \
                    get_config(config_b, second_level))

    differences = 0
    differing_field = None
    differing_entries = frozenset()

    for field in all_fields.get(second_level, 'general'):
        # If the field is undefined, we don't need to try to compare
        if field not in get_config(config_a, second_level):
            continue

            if get_config(config_a, second_level)[field] != \
               get_config(config_b, second_level)[field]:
                differences += 1
                differing_field = field
                differing_entries.add(get_config(config_a, second_level)[field])
                differing_entries.add(get_config(config_b, second_level)[field])

    return (differences, differing_field, differing_entries)

def compare_two_configs(config_a, config_b):

    # track the number of fields that differ and
    # keep track of what those differences are
    differences_top, differing_fields_top, differing_entries_top = \
      compare_two_configs_single_level(config_a, config_b)
    differences_second, differing_fields_second, differing_entries_second = \
      compare_two_configs_single_level(config_a, config_b, 'attack_properties')

    differences = differences_top + differences_second
    # If this assertion fails, it means that two simulations were
    # exactly the same, which should never happen
    assert differences >= 1
            
    if differences > 1:
        # If a pair of configs differ by more than one field, then we don't
        # want to plot them against each other
        pass
    elif differences == 1:
        # If the two configs only differ by one field, then they should not both
        # report differences
        assert differing_fields_top ^ differing_fields_second

        differing_field = differing_fields_top or differing_fields_second
        
        label_top_level = generate_label(config_a, differing_field, bucket_fields)
        label_second_level = generate_label(config_a, differing_field, attack_property_fields)
        label = "-".join([label_top_level, label_second_level])
        bucket = {'sensitivity_to':differing_field, 'label':label,\
                  'entries':differing_entries}
        if label not in buckets:
            buckets[label] = []
            buckets[label].append(bucket)


def generate_label(config, differing_field, label_set):
    "-".join([field + get_config(config, second_level)[field] \
                          for field in bucket_fields \
                          if (differing_field != field) and \
                          (field in get_config(config, second_level))])

# Bucket configs that differ in only one dimension, and also determine what
# that dimension is, and what the variations are.
def configs_to_buckets(configs, buckets={}):
    # There won't be too many simulations per experiment, so brute force is
    # fine. Iterate through all configs, skipping the last one, because the last
    # one would aready have been compared to each of the others
    for i, config_a in enumerate(configs[:-1]):
        
        # Compare config_a to each remaining config
        for config_b in configs[i+1:]:
            compare_two_configs(config_a, config_b)


# convert to paths
def config_to_paths(config):
    data_path = simulation_common.get_dynamic_data_path_config(config)
    base = simulation_common.get_output_base_path(config)
    print(f"{simulation_common.data_root}/{base}")
    print(data_path)

# plot all possible combinations of sets
# put plots in a pdf

if __name__ == "__main__":
    experiment_name = sys.argv[1]
    all_configs = experiments.get_experiment_config(experiment_name)
    buckets = {}
    for config_sets in all_configs:
        buckets = configs_to_buckets(config_sets, buckets)


    #     buckets = configs_to_buckets(config_sets)
    #     if buckets == None:
    #         # these two configs won't be plotted against each other, so just
    #         # move on
    #         pass

    #     config_to_paths(config)

    for b in buckets[0:]:
        print(b)
